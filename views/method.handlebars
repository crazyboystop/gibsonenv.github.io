<main id="main">
  <!--==========================
    About Us Section
  ============================-->
  <section id="method">
    <div class = "container">
      <div class="row method-container">

        <h2 class="title">Embodied Real-World Active Perception</h2>
        <div class="section-header">
          <h3 class="section-title">Introduction</h3>
          <p class="section-description"> <b>Perception and being active</b> (i.e. having a certain level of motion freedom) are closely tied. Learning active perception and sensorimotor control in the physical world is cumbersome as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning in simulation which consequently casts a question on transferring to real-world. In this paper, we study learning perception for active agents in real-world, propose a virtual environment for this purpose, and demonstrate complex learned locomotion abilities. The primary characteristics of the learning environments, which transfer into the trained agents, are I) being from the real-world and reflecting its semantic complexity, II) having a mechanism to ensure no need to further domain adaptation prior to deployment of results in real-world, III) embodiment of the agent and making it subject to constraints of space and physics. </p>
        </div>

        <div class="section-header">
          <h3 class="section-title">Publication</h3>
          <p class="section-intro">Checkout our <a href="#method">paper</a>, <a href="#method">supplementary material</a>, <a href="#method">bibtex</a>, <a href="#method">poster</a>. </p>
          <img src="/static/img/paper.jpg" />
          <p class="section-description">
            The paper is in submission to CVPR 2018. Feel free to contact us for more information.
          </p>

        </div>

        <div class="section-header">
          <h3 class="section-title">View Synthesis</h3>
          <p class="section-intro">As our focus is on perception, it is important to provide agents with high-quality visual inputs. Our view synthesis module takes a set of point clouds and synthesizes a novel view from an arbitrary viewpoint.</p>
          <p class="section-description">
          </p>
          <h3 class="section-second-title">Point Cloud Rendering</h3>
          <div class="col-12 text-center">
            <img class="img-responsive" width="100%" src="/static/img/method/fig1.png" />
          </div>
          <p class="section-description">
            There are a couple major issues of using real-world scanning data. First, the scans are sparse, meaning that a certain extent of view interpolation is required. Second, the quality of depth image and 3D mesh are limited to equipment and reconstruction algorithms. Details such as vegetation and small objects cannot be properly reconstructed most of the time. Reflective surfaces, such as windows and countertops will leave holes on the reconstructed mesh. All these factors prevent us from directly rendering from the meshes. 

            We Instead adopt a two-stage approach, with the first stage being rendering point clouds. Our CUDA point cloud renderer is neural network-friendly, and can render 1024x2048 video streams with 10M points at 80fps. Details of our renderer can be found in the paper.
          </p>
          <h3 class="section-second-title">Neural Network Based Rendering</h3>
          <div class="col-12 text-center">
            <img class="img-responsive" width="100%" src="/static/img/method/fig2.jpg" />
          </div>

          <p class="section-description">
            A couple of imperfections can be found in the using only point cloud rendering, including stitching marks, poor depth sensor output, and lighting inconsistencies. We use a neural network to do geometry fixes and feature based domain adaptation. 

            The architecture and hyperparameters of our convolutional neural network filler are detailed in supplementary materials. There are some key novelties for this part. First, our neural network utilizes dilated convolutions to aggregate contextual information. Second, our network runs at much faster speed than real time. Details of our neural network architecture can be found in the paper.
          </p>
          
        </div>


        <div class="section-header">
          <h3 class="section-title">Domain Adaptation: Goggles</h3>
          <p class="section-intro">Adapt to real world. </p>
          <div class="offset-2 col-8 text-center">
            <img class="img-responsive" width="100%" src="/static/img/figure4.jpg" />
          </div>
          <p class="section-description">
          </p>
        </div>

        <div class="section-header">
          <h3 class="section-title">Reinforcement Learning</h3>
          <p class="section-intro">Results of reinforcement learning achieved in our environment. </p>
          <div class="offset-2 col-8 text-center">
            <img class="img-responsive" width="100%" src="/static/img/figure4.jpg" />
          </div>
          <p class="section-description">
          </p>
        </div>

      </div>
    </div>
  </section><!-- #team -->


</main>
